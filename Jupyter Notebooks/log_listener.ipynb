{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b744072"
      },
      "source": [
        "# Log File - Data Transformation and Ingestion"
      ],
      "id": "7b744072"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64a23055"
      },
      "source": [
        "### Loading Libraries\n",
        "##### Spark Session, Dataframe Functions, Data types and Json"
      ],
      "id": "64a23055"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80af93e7"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import json"
      ],
      "id": "80af93e7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "767ea198"
      },
      "source": [
        "### Variables Initialization"
      ],
      "id": "767ea198"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40cb44f0"
      },
      "source": [
        "cassandra_host = \"cassandra\"\n",
        "cassandra_user = \"cassandra\"\n",
        "cassandra_pwd  = \"cassandra\"\n",
        "cassandra_port = 9042\n",
        "key_space      = \"LogAnalysis\"\n",
        "table_name     = \"NASALog\"\n",
        "kafka_server   = \"kafka:9092\"\n",
        "kafka_topic    = \"nasa_logs_demo\""
      ],
      "id": "40cb44f0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35080810"
      },
      "source": [
        "### Spark Session\n",
        "##### Spark Session object creation with configuration data stax spark-cassandra connector and cassandra related connectivity credentials."
      ],
      "id": "35080810"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5c7834e"
      },
      "source": [
        "#Spark Session creation configured to interact with Cassandra\n",
        "spark = SparkSession.builder.appName(\"pyspark-notebook\").\\\n",
        "config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0,com.datastax.spark:spark-cassandra-connector_2.12:3.0.0,com.datastax.spark:spark-cassandra-connector-driver_2.12:3.0.0\").\\\n",
        "config(\"spark.cassandra.connection.host\",cassandra_host).\\\n",
        "config(\"spark.cassandra.auth.username\",cassandra_user).\\\n",
        "config(\"spark.cassandra.auth.password\",cassandra_pwd).\\\n",
        "getOrCreate()"
      ],
      "id": "f5c7834e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83dbc20c"
      },
      "source": [
        "### Get data from Kafka with Schema\n",
        "##### Read data from Kafka topic via Spark structured streaming API by providing Kafka server and Topic details.\n",
        "##### Split the line into several fields "
      ],
      "id": "83dbc20c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94a3fd3d"
      },
      "source": [
        "#Read data from Kafka topic\n",
        "split_logic = split(col(\"url\"),\"\\.\").getItem(1)\n",
        "log_data = spark\\\n",
        "  .readStream\\\n",
        "  .format(\"kafka\")\\\n",
        "  .option(\"kafka.bootstrap.servers\",kafka_server)\\\n",
        "  .option(\"subscribe\", kafka_topic)\\\n",
        "  .option(\"startingOffsets\", \"earliest\")\\\n",
        "  .load()\\\n",
        "  .selectExpr(\"split(value,',')[1] as host\",\n",
        "              \"split(value,',')[2] as time\",\n",
        "              \"split(value,',')[3] as method\",\n",
        "              \"split(value,',')[4] as url\",\n",
        "              \"split(value,',')[5] as response\",\n",
        "              \"split(value,',')[6] as bytes\"\n",
        "             )\\\n",
        "  .withColumn(\"time_added\",unix_timestamp())\\ # Add new column\n",
        "  .withColumn(\"extension\",when(split_logic.isNull(),\"None\").otherwise(split_logic))"
      ],
      "id": "94a3fd3d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51896e4d"
      },
      "source": [
        "### Foreach Batch method\n",
        "##### This method is called from Spark foreachBatch sink and writes to cassandra database. It takes micro batch(dataframe) and its unique id as input."
      ],
      "id": "51896e4d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a44e872"
      },
      "source": [
        "def process_row(df, batch_id):\n",
        "    \"\"\"Writes data to Cassandra and HDFS location\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : DataFrame\n",
        "        Streaming Dataframe\n",
        "    batch_id : int\n",
        "        Unique id for each micro batch\n",
        "    \"\"\"\n",
        "    df.write\\\n",
        "    .format(\"org.apache.spark.sql.cassandra\")\\\n",
        "    .mode('append')\\\n",
        "    .options(table=\"nasalog\", keyspace=\"loganalysis\")\\\n",
        "    .save() #hot path (Speed layer)\n",
        "    df.write.csv(\"hdfs://namenode:8020/output/nasa_logs/\",mode=\"append\") #cold path (Batch layer)"
      ],
      "id": "6a44e872",
      "execution_count": null,
      "outputs": []
    }
  ]
}